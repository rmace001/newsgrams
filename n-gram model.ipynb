{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# from collections import Counter\n",
    "# from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "porter = PorterStemmer()\n",
    "# #########################\n",
    "# DATA CLEANING TECHNIQUES\n",
    "# #########################\n",
    "# split by sentance\n",
    "# remove puncuation\n",
    "# lowercase letters\n",
    "# remove/convert numbers\n",
    "# -------------------------\n",
    "\n",
    "# #########################\n",
    "# NLP TECHNIQUES\n",
    "# #########################\n",
    "# stemming\n",
    "# -------------------------\n",
    "# sent1 = \"i am sam\"\n",
    "# sent2 = \"sam i am\"\n",
    "# sent3 = \"i do not like green eggs and ham\"\n",
    "# sent4 = \"the quick person did not realize his speed and the quick person bumped \"\n",
    "\n",
    "\n",
    "# # words = re.findall(\"\\w+\", sent4)\n",
    "# # bigramCount = dict(Counter(zip(words, islice(words, 1, None))))\n",
    "\n",
    "# # for i in bigramCount.keys():\n",
    "# #     print(str(i) + \" count: %d\" % bigramCount[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param q: the column name of the query article\n",
    "# This is when calculating between vectors of the same length\n",
    "def CosineScore(q):\n",
    "    scores = {}\n",
    "    query = weightmatrix.loc[:,q].values\n",
    "    normq = (query@query)\n",
    "    for doc in doc_index.keys():\n",
    "        if not doc == q:\n",
    "            # divide by the values of the weight vector norms\n",
    "            d = weightmatrix.loc[:,doc].values\n",
    "            normd = d@d\n",
    "            scores[doc] = (query@d)/((normq*normd)**0.5)\n",
    "    sorted_scores = sorted(scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return sorted_scores[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "############## all clean and ready for cross-val ################\n"
     ]
    }
   ],
   "source": [
    "stopwords = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"that\", \"thats\", \"that's\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\", \"now\"}\n",
    "gibberish = ['\\(CNN\\)', '\\\\\\\\\\\\\\'', '\\'', 'Read More', 'CNN', 'Getty Images', '\\\\\\\\xc2', '\\\\\\\\xb0', '\\\\\\\\xc3', '\\\\\\\\x94', '\\\\\\\\x9c', '\\\\\\\\x9d', '\\\\\\\\x95', '\\\\\\\\xb3', '\\\\\\\\xe2', '\\\\\\\\x80', '\\\\\\\\x99', '\\\\\\\\xa9', '\\\\\\\\xf0', '\\\\\\\\x9f', '\\\\\\\\x91']\n",
    "collection = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# load in the crawled data and perform some cleaning\n",
    "fullnews = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\covidLabeled.csv\")\n",
    "# print(news.shape[0])\n",
    "fullnews = fullnews[:50]\n",
    "\n",
    "newsdict = {}\n",
    "newsdict['headline'] = fullnews.loc[:, 'headline'].values\n",
    "newsdict['body'] = fullnews.loc[:, 'body'].values\n",
    "newsdict['covid labels'] = fullnews.loc[:, 'covid labels'].values\n",
    "\n",
    "\n",
    "news = pd.DataFrame.from_dict(newsdict)\n",
    "newsdict = ''\n",
    "# for i in range(news.shape[0]):\n",
    "#     s = news.iloc[i][3][2:]\n",
    "#     s2 = news.iloc[i][1][2:]\n",
    "#     news.at[i,'body'] = s\n",
    "#     news.at[i,'headline'] = s2\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "for i in range(len(gibberish)):\n",
    "    for j in range(news.shape[0]):\n",
    "        output = news.iloc[j][1]\n",
    "        output = re.sub(gibberish[i], '', output)\n",
    "        output = re.sub('\\\\\\\\n', ' ', output)\n",
    "        output = re.sub('-', ' ', output)\n",
    "        news.at[j,'body'] = output\n",
    "\n",
    "print(\"############## all clean and ready for cross-val ################\")\n",
    "\n",
    "gibberish = ''\n",
    "fullnews = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantokens(news):\n",
    "    dterms = {}\n",
    "    doc_labels = {}\n",
    "    for j in range(news.shape[0]):\n",
    "        tokens = word_tokenize(news.iloc[j][1])\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        depunc = [t.translate(collection) for t in tokens]\n",
    "        words = [w for w in depunc if not w  == '']\n",
    "        cleaner = [w for w in words if not w in stopwords]\n",
    "        stemmed = [porter.stem(word) for word in cleaner]\n",
    "        dterms[str(j)+news.iloc[j][0]] = stemmed\n",
    "        doc_labels[str(j)+news.iloc[j][0]] = news.iloc[j][2]\n",
    "    return dterms, doc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that creates termlist and doc_idx as we count each doc_length\n",
    "def populate_data_structs(dterms, term_list, doc_index):\n",
    "    for doc in dterms.keys():\n",
    "        term_list += [(word, doc) for word in dterms[doc]]\n",
    "        doc_index[doc] = len(dterms[doc])\n",
    "\n",
    "def make_term_index(term_list, term_index):\n",
    "    def update_term(item):\n",
    "        i = 0\n",
    "        for listitem in term_index[item[0]]:\n",
    "            # if term already found in doc\n",
    "            if (item[1] == listitem[0]):\n",
    "                listitem[1] += 1\n",
    "                return\n",
    "            else:\n",
    "                i += 1\n",
    "        term_index[item[0]].append( [item[1] , 1 ] )\n",
    "    \n",
    "    for item in term_list:\n",
    "        if item[0] not in term_index:\n",
    "            # add term\n",
    "            term_index[item[0]] = [[item[1], 1]]\n",
    "        else:\n",
    "            # update term\n",
    "            update_term(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_score(query_words, doc_index, term_index):\n",
    "    def get_q_weight(term):\n",
    "        idf = 1 + math.log(len(doc_index)/len(term_index[term])) if term in term_index else 1\n",
    "        return (query_words.count(term) / len(query_words)) * idf\n",
    "    \n",
    "#     def get_d_weight(term, pair):\n",
    "#         if term in term_index:\n",
    "#             return (pair[1]/doc_index[pair[0]]) * (1 + math.log(len(doc_index)/len(term_index[term])))\n",
    "#         else:\n",
    "#             return 0\n",
    "\n",
    "    scores = {key: 0 for key in doc_index.keys()}\n",
    "    for term in query_words:\n",
    "        for pair in term_index[term] if term in term_index else []:\n",
    "#             scores[pair[0]] += get_q_weight(term) * get_d_weight(term, pair)\n",
    "            scores[pair[0]] += get_q_weight(term) * (pair[1]/doc_index[pair[0]]) * (1 + math.log(len(doc_index)/len(term_index[term])))\n",
    "    scores = {doc: scores[doc] / (doc_index[doc]*len(query_words)) for doc in doc_index.keys()}\n",
    "    return sorted(scores.items(), key = operator.itemgetter(1), reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate 1\n",
      "Validate 2\n",
      "Validate 3\n",
      "Validate 4\n",
      "Validate 5\n",
      "Validate 6\n",
      "Validate 7\n",
      "Validate 8\n",
      "Validate 9\n",
      "Validate 10\n",
      "Average error per validation split: 50.000000%\n"
     ]
    }
   ],
   "source": [
    "splits = 10\n",
    "pointspersplit = 5\n",
    "covidlabels = {}\n",
    "three_nearest_neighbors = {}\n",
    "errors = []\n",
    "for i in range(splits):\n",
    "    end = pointspersplit*(i+1)\n",
    "    start = end - pointspersplit\n",
    "    valid = news.iloc[start:end][:]\n",
    "    train = pd.concat([news.iloc[:start][:], news.iloc[end:][:]])\n",
    "    dterms, dlabels = cleantokens(train)\n",
    "    valid_dterms, valid_doc_labels = cleantokens(valid)\n",
    "    term_list = []\n",
    "    doc_index = {}\n",
    "    populate_data_structs(dterms, term_list, doc_index)\n",
    "    term_index = {}\n",
    "    make_term_index(term_list, term_index)\n",
    "    error = 0\n",
    "    print('Validate %d' % (i+1))\n",
    "    for q in valid_dterms.keys():\n",
    "        covidlabels['quarantine'] = 0\n",
    "        covidlabels['social distancing'] = 0\n",
    "        covidlabels['treatment'] = 0\n",
    "        covidlabels['economic effects'] = 0\n",
    "        covidlabels['pandemic'] = 0\n",
    "        labels = []\n",
    "        scores = cosine_score(valid_dterms[q], doc_index, term_index)\n",
    "        for j in range(len(scores)):\n",
    "            labels.append(dlabels[scores[j][0]]) # append each label from closest to farthest\n",
    "            covidlabels[dlabels[scores[j][0]]] += 1\n",
    "        top2 = sorted(covidlabels.items(), key = operator.itemgetter(1), reverse = True)[:2]\n",
    "        max_label = ''\n",
    "        if top2[0][1] == top2[1][1]:\n",
    "            # pick the closer one\n",
    "            for l in labels:\n",
    "                if l == top2[0][0] or l == top2[1][0]:\n",
    "                    max_label = top2[0][0] if l == top2[0][0] else top2[1][0] \n",
    "                    break\n",
    "        else:\n",
    "            max_label = top2[0][0]\n",
    "        if not max_label == valid_doc_labels[q]:\n",
    "            error += 1\n",
    "        three_nearest_neighbors[q] = [scores, max_label]\n",
    "    errors.append(error)\n",
    "    \n",
    "print('Average error per validation split: %f%%' % (np.mean(errors)*100/pointspersplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df = pd.DataFrame.from_dict(three_nearest_neighbors, orient='index', columns=['3-NN', 'Label'])\n",
    "# df.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\ThreeNN.csv', index = True, header=True)\n",
    "\n",
    "print(len(trainsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign labels for articles using best matching keyword set \n",
    "keywords = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\labelandKeywords.csv\")\n",
    "print(keywords.shape[0])\n",
    "display(keywords.head())\n",
    "label_keywords = {}\n",
    "for j in range(keywords.shape[0]):\n",
    "    tokens = word_tokenize(keywords.iloc[j][1])\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    cleaner = [w for w in tokens if not w in stopwords]\n",
    "    stemmed = [porter.stem(word) for word in cleaner]\n",
    "    label_keywords[keywords.iloc[j][0]] = stemmed\n",
    "    \n",
    "dterms, doc_labels = cleantokens(news)\n",
    "article_covid_labels = []\n",
    "words_in_labels = {}\n",
    "q_index = {}\n",
    "qterm_list = []\n",
    "qterm_index = {}\n",
    "populate_data_structs(label_keywords, words_in_labels, qterm_list, q_index)\n",
    "make_term_index(qterm_list, qterm_index)\n",
    "\n",
    "for doc in dterms.keys():\n",
    "    scores = cosine_score(dterms[doc], q_index, qterm_index)\n",
    "    article_covid_labels.append(scores[0][0])\n",
    "\n",
    "news['covid labels']  = article_covid_labels\n",
    "\n",
    "# news.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\covidLabeld.csv', index = False, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termweight(term, docno):\n",
    "    if term in words_in_doc[docno]:\n",
    "        for postings in term_index[term]:\n",
    "            if postings[0] == docno:\n",
    "                return postings[1]/doc_index[postings[0]] * math.log(len(doc_index)/len(term_index[term]), 2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "term_doc_matrix = {}\n",
    "\n",
    "for term in term_index.keys():\n",
    "    row = []\n",
    "    for doc in doc_index.keys():\n",
    "        row.append(termweight(term, doc))\n",
    "    term_doc_matrix[term] = row\n",
    "\n",
    "# keys = list(doc_index.keys())\n",
    "# df = pd.DataFrame.from_dict(term_doc_matrix, orient='index', columns=keys)\n",
    "# df.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\TFIDFTrainSet.csv', index = True, header=True)\n",
    "\n",
    "term_list = []\n",
    "# doc_index = {}\n",
    "# term_index = {}\n",
    "words_per_doc = {}\n",
    "# term_doc_matrix = {}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
