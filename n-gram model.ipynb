{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "porter = PorterStemmer()\n",
    "# #########################\n",
    "# DATA CLEANING TECHNIQUES\n",
    "# #########################\n",
    "# split by sentance\n",
    "# remove puncuation\n",
    "# lowercase letters\n",
    "# remove/convert numbers\n",
    "# -------------------------\n",
    "\n",
    "# #########################\n",
    "# NLP TECHNIQUES\n",
    "# #########################\n",
    "# stemming\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stopwords = {\"I\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"that\", \"thats\", \"that's\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\", \"now\"}\n",
    "# sent1 = \"i am sam\"\n",
    "# sent2 = \"sam i am\"\n",
    "# sent3 = \"i do not like green eggs and ham\"\n",
    "# sent4 = \"the quick person did not realize his speed and the quick person bumped \"\n",
    "\n",
    "\n",
    "# # words = re.findall(\"\\w+\", sent4)\n",
    "# # bigramCount = dict(Counter(zip(words, islice(words, 1, None))))\n",
    "\n",
    "# # for i in bigramCount.keys():\n",
    "# #     print(str(i) + \" count: %d\" % bigramCount[i])\n",
    "\n",
    "# load in the crawled data and perform some cleaning\n",
    "news = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\newsutf8.csv\")\n",
    "print(news.shape[0])\n",
    "news = news[:200]\n",
    "for i in range(news.shape[0]):\n",
    "    s = news.iloc[i][4][2:]\n",
    "    news.at[i,'body'] = s\n",
    "# print(news.iloc[1][4])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n",
      " Now thats one helluva Costco run.  A grocer on a remote Alaskan island thats only accessible by boat or plane is going above and beyond to keep his small city fed during the coronavirus pandemic.  Toshua Parker, owner of Icy Strait Wholesale in Gustavus, has been making a weekly boat trip with his staff to Juneau, the states capital 50 miles away, to restock on essential food and supplies at Costco. It takes them about 14 hours to complete the journey on a 96 feet long converted military landing craft.  Gustavus is a coastal community that borders Glacier Bay National Park. For the citys 450 residents, the only place to buy groceries is ToshCo, the locals nickname for Parkers store.  Parker usually has food and supplies shipped from Costco to his store aboard the states ferry system, but its no longer running to Gustavus because of the pandemic, as well as damage caused to the citys dock by severe storms.  Read More\"\n"
     ]
    }
   ],
   "source": [
    "gibberish = ['\\(CNN\\)', 'CNN', 'Getty Images', '\\\\\\\\xc3', '\\\\\\\\x94', '\\\\\\\\x9c', '\\\\\\\\x9d', '\\\\\\\\x95', '\\\\\\\\xb3', '\\\\\\\\xe2', '\\\\\\\\x80', '\\\\\\\\x99', '\\\\\\\\xa9', '\\\\\\\\xf0', '\\\\\\\\x9f', '\\\\\\\\x91']\n",
    "for i in range(len(gibberish)):\n",
    "    for j in range(news.shape[0]):\n",
    "        output = news.iloc[j][4]\n",
    "        output = re.sub(gibberish[i], '', output)\n",
    "        output = re.sub('\\\\\\\\n', ' ', output)\n",
    "        output = re.sub('\\\\\\\\\\\\\\'', '', output)\n",
    "        output = re.sub('\\'', '', output)\n",
    "        news.at[j,'body'] = output\n",
    "\n",
    "print(\"#################################################\")\n",
    "# for i in range(news.shape[0]):\n",
    "#     print(news.at[i, 'body'])\n",
    "#     print(\"#################################################\")\n",
    "print(news.iloc[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = str.maketrans('', '', string.punctuation)\n",
    "dterms = dict()\n",
    "for j in range(news.shape[0]):    \n",
    "    tokens = word_tokenize(news.iloc[j][4])\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    depunc = [t.translate(collection) for t in tokens]\n",
    "    words = [w for w in depunc if not w  == '']\n",
    "    cleaner = [w for w in words if not w in stopwords]\n",
    "    stemmed = [porter.stem(word) for word in cleaner]\n",
    "    dterms[j] = stemmed\n",
    "#     dterms[j] = cleaner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = []\n",
    "doc_index = {}\n",
    "term_index = {}\n",
    "\n",
    "\n",
    "\n",
    "#function that creates termlist and doc_idx as we count each doc_length\n",
    "def make_term_list_and_doc_index():\n",
    "    for doc in range(len(dterms)):\n",
    "        count = 0\n",
    "        for word in dterms[doc]:\n",
    "            count += 1\n",
    "            term_list.append((word, doc))\n",
    "        doc_index[doc] = count\n",
    "\n",
    "def update_term(item):\n",
    "    i = 0\n",
    "    for listitem in term_index[item[0]]:\n",
    "        #if word_docName == encounterd_fileID\n",
    "        if (item[1] == listitem[0]):\n",
    "            listitem[1] += 1\n",
    "            return\n",
    "        else:\n",
    "            i += 1\n",
    "    term_index[item[0]].append( [item[1] , 1 ] )\n",
    "    return\n",
    "\n",
    "\n",
    "def make_term_index():\n",
    "    for item in term_list:\n",
    "        if item[0] not in term_index:\n",
    "            # add term\n",
    "            term_index[item[0]] = [[item[1], 1]]\n",
    "        else:\n",
    "            # update term\n",
    "            update_term(item)\n",
    "            \n",
    "make_term_list_and_doc_index()\n",
    "make_term_index()\n",
    "# i = 0\n",
    "# print(len(term_index))\n",
    "# for key, value in term_index.items(): \n",
    "#     print(key, \":\", value)\n",
    "#     if i > 40:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_data(term):\n",
    "    print(term + ' -> ')\n",
    "    if term in term_index:\n",
    "        i = 0\n",
    "        for postings in term_index[term]:\n",
    "            tf = postings[1]/doc_index[postings[0]] #num_occur/doc_size\n",
    "            idf = math.log(len(doc_index)/len(term_index[term]), 2)\n",
    "            tf_idf = tf*idf\n",
    "            print('Posting: Doc# ' + str(postings[0]) + ', tf: ' + str(tf) + ', idf: ' + str(idf) + ', tf-idf: ' + str(tf_idf))\n",
    "            if i > 14:\n",
    "                break\n",
    "            i += 1\n",
    "    else:\n",
    "        print('Term not found')\n",
    "        \n",
    "# i = 0\n",
    "# for key, value in term_index.items(): \n",
    "#     print(key)\n",
    "#     print(len(term_index[key]))\n",
    "#     if i > 34:\n",
    "#         break\n",
    "#     i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the set and the dataframe with the first word of the first document\n",
    "# words_encountered = set(dterms[0][0])\n",
    "# unigrams = pd.DataFrame({dterms[0][0]: [0]})\n",
    "\n",
    "# for i in range(news.shape[0]):\n",
    "#     if i > 0:\n",
    "#         tdf = pd.DataFrame([[0]*len(unigrams.columns)], columns=unigrams.columns)\n",
    "#         t = unigrams.append(tdf,ignore_index=True)\n",
    "#         unigrams = t\n",
    "#     for word in dterms[i]:\n",
    "#         if word not in words_encountered:\n",
    "#             words_encountered.add(word)\n",
    "#             col_index = len(unigrams.columns)\n",
    "#             temp = pd.DataFrame({word: [0]*(i+1)})\n",
    "#             t2 = unigrams.join(temp)\n",
    "#             unigrams = t2\n",
    "#             unigrams.at[i, word] = 1\n",
    "#         else:\n",
    "#             cols = list(unigrams.columns)\n",
    "#             unigrams.at[i, word] += 1\n",
    "# too slow with dataframes ^ \n",
    "\n",
    "# for i in range(10):\n",
    "#     print(dterms[i][:6])\n",
    "\n",
    "# unigrams.to_csv('unigrams2.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in dterms.keys():\n",
    "#     print(k, dterms[k][0])\n",
    "\n",
    "a_file = open(\"TermDocMatrix.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in term_index.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "a_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
