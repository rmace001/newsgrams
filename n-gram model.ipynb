{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# from collections import Counter\n",
    "# from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "porter = PorterStemmer()\n",
    "# #########################\n",
    "# DATA CLEANING TECHNIQUES\n",
    "# #########################\n",
    "# split by sentance\n",
    "# remove puncuation\n",
    "# lowercase letters\n",
    "# remove/convert numbers\n",
    "# -------------------------\n",
    "\n",
    "# #########################\n",
    "# NLP TECHNIQUES\n",
    "# #########################\n",
    "# stemming\n",
    "# -------------------------\n",
    "# sent1 = \"i am sam\"\n",
    "# sent2 = \"sam i am\"\n",
    "# sent3 = \"i do not like green eggs and ham\"\n",
    "# sent4 = \"the quick person did not realize his speed and the quick person bumped \"\n",
    "\n",
    "\n",
    "# # words = re.findall(\"\\w+\", sent4)\n",
    "# # bigramCount = dict(Counter(zip(words, islice(words, 1, None))))\n",
    "\n",
    "# # for i in bigramCount.keys():\n",
    "# #     print(str(i) + \" count: %d\" % bigramCount[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7009\n",
      "done\n",
      "############## all clean and ready for cross-val ################\n"
     ]
    }
   ],
   "source": [
    "stopwords = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"that\", \"thats\", \"that's\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\", \"now\"}\n",
    "gibberish = ['\\(CNN\\)', 'CNN', 'Getty Images', '\\\\\\\\xc2', '\\\\\\\\xb0', '\\\\\\\\xc3', '\\\\\\\\x94', '\\\\\\\\x9c', '\\\\\\\\x9d', '\\\\\\\\x95', '\\\\\\\\xb3', '\\\\\\\\xe2', '\\\\\\\\x80', '\\\\\\\\x99', '\\\\\\\\xa9', '\\\\\\\\xf0', '\\\\\\\\x9f', '\\\\\\\\x91']\n",
    "collection = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# load in the crawled data and perform some cleaning\n",
    "news = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\final.csv\")\n",
    "print(news.shape[0])\n",
    "# news = news[:50]\n",
    "\n",
    "# for i in range(news.shape[0]):\n",
    "#     s = news.iloc[i][3][2:]\n",
    "#     s2 = news.iloc[i][1][2:]\n",
    "#     news.at[i,'body'] = s\n",
    "#     news.at[i,'headline'] = s2\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "for i in range(len(gibberish)):\n",
    "    for j in range(news.shape[0]):\n",
    "        output = news.iloc[j][4]\n",
    "        output = re.sub(gibberish[i], '', output)\n",
    "        output = re.sub('\\\\\\\\n', ' ', output)\n",
    "        output = re.sub('\\\\\\\\\\\\\\'', '', output)\n",
    "        output = re.sub('\\'', '', output)\n",
    "        output = re.sub('-', ' ', output)\n",
    "        output = re.sub('Read More', '', output)\n",
    "        news.at[j,'body'] = output\n",
    "\n",
    "print(\"############## all clean and ready for cross-val ################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantokens(news):\n",
    "    dterms = {}\n",
    "    doc_labels = {}\n",
    "    for j in range(news.shape[0]):\n",
    "        tokens = word_tokenize(news.iloc[j][4])\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        depunc = [t.translate(collection) for t in tokens]\n",
    "        words = [w for w in depunc if not w  == '']\n",
    "        cleaner = [w for w in words if not w in stopwords]\n",
    "        stemmed = [porter.stem(word) for word in cleaner]\n",
    "        dterms[str(j)+news.iloc[j][2]] = stemmed\n",
    "        doc_labels[str(j)+news.iloc[j][2]] = news.iloc[j][6]\n",
    "    #     dterms[j] = cleaner\n",
    "    return dterms, doc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_labels = {}\n",
    "# dterms = {}\n",
    "# term_list = []\n",
    "# doc_index = {}\n",
    "# term_index = {}\n",
    "# words_in_doc = {}\n",
    "\n",
    "# valid_doc_labels = {}\n",
    "# valid_dterms = {}\n",
    "# valid_term_list = []\n",
    "# valid_doc_index = {}\n",
    "# valid_term_index = {}\n",
    "# valid_words_in_doc = {}\n",
    "\n",
    "#function that creates termlist and doc_idx as we count each doc_length\n",
    "def populate_data_structs(dterms, words_in_doc, term_list, doc_index):\n",
    "    for doc in dterms.keys():\n",
    "        count = 0\n",
    "        words_in_doc[doc] = set({})\n",
    "        for word in dterms[doc]:\n",
    "            count += 1\n",
    "            term_list.append((word, doc))\n",
    "            words_in_doc[doc].add(word)\n",
    "        doc_index[doc] = count\n",
    "\n",
    "\n",
    "def make_term_index(term_list, term_index):\n",
    "    def update_term(item):\n",
    "        i = 0\n",
    "        for listitem in term_index[item[0]]:\n",
    "            #if word_docName == encounterd_fileID\n",
    "            if (item[1] == listitem[0]):\n",
    "                listitem[1] += 1\n",
    "                return\n",
    "            else:\n",
    "                i += 1\n",
    "        term_index[item[0]].append( [item[1] , 1 ] )\n",
    "    \n",
    "    for item in term_list:\n",
    "        if item[0] not in term_index:\n",
    "            # add term\n",
    "            term_index[item[0]] = [[item[1], 1]]\n",
    "        else:\n",
    "            # update term\n",
    "            update_term(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_score(query_words, doc_index, term_index):\n",
    "    def get_posting_list(term):\n",
    "        return term_index[term] if term in term_index else []\n",
    "    \n",
    "    def get_q_weight(term):\n",
    "        tf = 0\n",
    "        for w in query_words:\n",
    "            if w == term:\n",
    "                tf += 1\n",
    "        tf = tf / len(query_words)\n",
    "        if term in term_index:\n",
    "            idf = 1 + math.log(len(doc_index)/len(term_index[term]))\n",
    "        else:\n",
    "            idf = 1\n",
    "        return tf * idf\n",
    "    \n",
    "    def get_d_weight(term, pair):\n",
    "        if term in term_index:\n",
    "            tf = pair[1]/doc_index[pair[0]] #num_occur/doc_size\n",
    "            idf = 1 + math.log(len(doc_index)/len(term_index[term]))\n",
    "            tf_idf = tf * idf\n",
    "        else:\n",
    "            tf_idf = 0\n",
    "        return tf_idf\n",
    "\n",
    "    scores = {}\n",
    "    for doc in doc_index.keys():\n",
    "        scores[doc] = 0\n",
    "    \n",
    "    for term in query_words:\n",
    "        for pair in get_posting_list(term):\n",
    "            scores[pair[0]] += get_q_weight(term) * get_d_weight(term, pair)\n",
    "    qlength = len(query_words)\n",
    "    for doc in doc_index.keys():\n",
    "        scores[doc] = scores[doc] / (doc_index[doc]*qlength)\n",
    "        \n",
    "    sorted_scores = sorted(scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return sorted_scores[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpotentials(qu, s, t, e, p, o, maxval):\n",
    "    pot = []\n",
    "    if qu == s and qu == maxval:\n",
    "        pot = ['quarantine', 'social distancing']\n",
    "    elif qu == t and qu == maxval:\n",
    "        pot = ['quarantine', 'treatment']\n",
    "    elif qu == p and qu == maxval:\n",
    "        pot = ['quarantine', 'pandemic']\n",
    "    elif qu == e and qu == maxval:\n",
    "        pot = ['quarantine', 'economic effects']\n",
    "    elif qu == o and qu == maxval:\n",
    "        pot = ['quarantine', 'other']\n",
    "    elif s == t and s == maxval:\n",
    "        pot = ['social distancing', 'treatment']\n",
    "    elif s == p and s == maxval:\n",
    "        pot = ['social distancing', 'pandemic']\n",
    "    elif s == e and s == maxval:\n",
    "        pot = ['social distancing', 'economic effects']\n",
    "    elif s == o and s == maxval:\n",
    "        pot = ['social distancing', 'other']\n",
    "    elif t == p and t == maxval:\n",
    "        pot = ['treatment', 'pandemic']\n",
    "    elif t == e and t == maxval:\n",
    "        pot = ['treatment', 'economic effects']\n",
    "    elif t == o and t == maxval:\n",
    "        pot = ['treatment', 'other']\n",
    "    elif p == e and p == maxval:\n",
    "        pot = ['pandemic', 'economic effects']\n",
    "    elif p == o and p == maxval:\n",
    "        pot = ['pandemic', 'other']\n",
    "    else:\n",
    "        pot = ['economic effects', 'other']\n",
    "    return pot\n",
    "\n",
    "\n",
    "three_nearest_neighbors = {}\n",
    "k = 3\n",
    "\n",
    "quarentine = 'quarantine'\n",
    "socialdist = 'social distancing'\n",
    "treat = 'treatment'\n",
    "econeff = 'economic effects'\n",
    "pandemic = 'pandemic'\n",
    "other = 'other'\n",
    "splits = 24\n",
    "pointspersplit = 5\n",
    "errors = []\n",
    "for i in range(splits):\n",
    "    end = pointspersplit*(i+1)\n",
    "    start = end - pointspersplit\n",
    "    valid = news.iloc[start:end][:]\n",
    "    train = pd.concat([news.iloc[:start][:], news.iloc[end:][:]])\n",
    "#     make all of these:\n",
    "    dterms, doc_labels = cleantokens(train)\n",
    "    term_list = []\n",
    "    doc_index = {}\n",
    "    words_in_doc = {}\n",
    "    populate_data_structs(dterms, words_in_doc, term_list, doc_index)\n",
    "    term_index = {}\n",
    "    make_term_index(term_list, term_index)\n",
    "    valid_dterms, valid_doc_labels = cleantokens(valid)\n",
    "    error = 0\n",
    "    \n",
    "    for q in valid_dterms.keys():\n",
    "        qu = s = t = e = p = o = 0\n",
    "        labels = []\n",
    "        scores = cosine_score(valid_dterms[q], doc_index, term_index)\n",
    "        for j in range(len(scores)):\n",
    "            label = doc_labels[scores[j][0]]\n",
    "            labels.append(label) # append each label from closest to farthest\n",
    "            if label == 'quarantine':\n",
    "                qu += 1\n",
    "            elif label == 'social distancing':\n",
    "                s += 1\n",
    "            elif label == 'treatment':\n",
    "                t += 1\n",
    "            elif label == 'economic effects':\n",
    "                e += 1\n",
    "            elif label == 'pandemic':\n",
    "                p += 1\n",
    "            else:\n",
    "                o += 1\n",
    "        top2 = sorted([qu, s, t, e, p, o], reverse = True)[:2]\n",
    "        max_label = ''\n",
    "        if top2[0] == top2[1]:\n",
    "            # pick the closer one\n",
    "            max_label = ''\n",
    "            potentials = getpotentials(qu, s, t, e, p, o, top2[0])\n",
    "            for l in labels:\n",
    "                if l == potentials[0] or l == potentials[1]:\n",
    "                    if l == potentials[0]:\n",
    "                        max_label = potentials[0]\n",
    "                    else:\n",
    "                        max_label = potentials[1]\n",
    "                    break\n",
    "        else:\n",
    "#             max_count = np.max([qu, s, t, e, p, o])\n",
    "            max_count = top2[0]\n",
    "            if max_count == qu:\n",
    "                max_label = 'quarantine'\n",
    "            elif max_count == s:\n",
    "                max_label = 'social distancing'\n",
    "            elif max_count == t:\n",
    "                max_label = 'treatment'\n",
    "            elif max_count == e:\n",
    "                max_label = 'economic effects'\n",
    "            elif max_count == p:\n",
    "                max_label = 'pandemic'\n",
    "            else:\n",
    "                max_label = 'other'\n",
    "        if not max_label == valid_doc_labels[q]:\n",
    "            error += 1\n",
    "        three_nearest_neighbors[q] = [scores, max_label]\n",
    "    errors.append(error)\n",
    "    \n",
    "print('Average error per validation split: %f%' % (np.mean(errors)/pointspersplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quarantine</td>\n",
       "      <td>Shelter In Place \\tWork From Home \\tRemote Edu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social distancing</td>\n",
       "      <td>Flatten The Curve \\tFace Masks \\tStay Home \\t6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>treatment</td>\n",
       "      <td>Emerging Data\\tClinical Trials\\tDetection\\tTes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>economic effects</td>\n",
       "      <td>Stock Market\\tincome\\tjob\\tlayoff\\tSmall Busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pandemic</td>\n",
       "      <td>Outbreak\\t Reopening\\tInfluenza\\tclosed\\tdeath...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label                                           keywords\n",
       "0         quarantine  Shelter In Place \\tWork From Home \\tRemote Edu...\n",
       "1  social distancing  Flatten The Curve \\tFace Masks \\tStay Home \\t6...\n",
       "2          treatment  Emerging Data\\tClinical Trials\\tDetection\\tTes...\n",
       "3   economic effects  Stock Market\\tincome\\tjob\\tlayoff\\tSmall Busin...\n",
       "4           pandemic  Outbreak\\t Reopening\\tInfluenza\\tclosed\\tdeath..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\labelandKeywords.csv\")\n",
    "print(keywords.shape[0])\n",
    "display(keywords.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_keywords = {}\n",
    "for j in range(keywords.shape[0]):\n",
    "    tokens = word_tokenize(keywords.iloc[j][1])\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    cleaner = [w for w in tokens if not w in stopwords]\n",
    "    stemmed = [porter.stem(word) for word in cleaner]\n",
    "    label_keywords[keywords.iloc[j][0]] = stemmed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dterms, doc_labels = cleantokens(news)\n",
    "# term_list = []\n",
    "# doc_index = {}\n",
    "# words_in_doc = {}\n",
    "# populate_data_structs(dterms, words_in_doc, term_list, doc_index)\n",
    "# term_index = {}\n",
    "# make_term_index(term_list, term_index)\n",
    "article_covid_labels = []\n",
    "\n",
    "words_in_labels = {}\n",
    "q_index = {}\n",
    "qterm_list = []\n",
    "qterm_index = {}\n",
    "populate_data_structs(label_keywords, words_in_labels, qterm_list, q_index)\n",
    "make_term_index(qterm_list, qterm_index)\n",
    "\n",
    "for doc in dterms.keys():\n",
    "    scores = cosine_score(dterms[doc], q_index, qterm_index)\n",
    "    article_covid_labels.append(scores[0][0])\n",
    "\n",
    "news['covid labels']  = article_covid_labels\n",
    "\n",
    "news.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\covidLabeld.csv', index = False, header=True)\n",
    "    \n",
    "# df = pd.DataFrame.from_dict(three_nearest_neighbors, orient='index', columns=['3-NN', 'Label'])\n",
    "# df.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\ThreeNN.csv', index = True, header=True)\n",
    "i = 0\n",
    "# for doc in dterms.keys():\n",
    "#     print(doc)\n",
    "#     print(article_covid_labels[i])\n",
    "#     i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termweight(term, docno):\n",
    "    if term in words_in_doc[docno]:\n",
    "        for postings in term_index[term]:\n",
    "            if postings[0] == docno:\n",
    "                return postings[1]/doc_index[postings[0]] * math.log(len(doc_index)/len(term_index[term]), 2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "term_doc_matrix = {}\n",
    "\n",
    "for term in term_index.keys():\n",
    "    row = []\n",
    "    for doc in doc_index.keys():\n",
    "        row.append(termweight(term, doc))\n",
    "    term_doc_matrix[term] = row\n",
    "\n",
    "\n",
    "# df = pd.DataFrame.from_dict(term_doc_matrix, orient='index', columns=keys)\n",
    "# df.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\TFIDFTrainSet.csv', index = True, header=True)\n",
    "\n",
    "term_list = []\n",
    "# doc_index = {}\n",
    "# term_index = {}\n",
    "words_per_doc = {}\n",
    "# term_doc_matrix = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate_data_structs()\n",
    "# keys = list(doc_index.keys())\n",
    "# make_term_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param q: the column index of the query article, or the column name of the query article, I think the latter would work\n",
    "def CosineScore(q):\n",
    "    scores = {}\n",
    "    query = weightmatrix.loc[:,q].values\n",
    "    normq = (query@query)\n",
    "    for doc in doc_index.keys():\n",
    "        if not doc == q:\n",
    "            # divide by the values of the weight vector norms\n",
    "            d = weightmatrix.loc[:,doc].values\n",
    "            normd = d@d\n",
    "            scores[doc] = (query@d)/((normq*normd)**0.5)\n",
    "    sorted_scores = sorted(scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return sorted_scores[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightmatrix = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\TFIDFMatrix.csv\")\n",
    "# weightmatrix = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\TFIDFTrainSet.csv\")\n",
    "# print(weightmatrix.shape)\n",
    "# display(weightmatrix.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(term_doc_matrix, orient='index', columns=['3-NN', 'Label'])\n",
    "# df.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\TFIDFTrainSet.csv', index = True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the set and the dataframe with the first word of the first document\n",
    "# words_encountered = set(dterms[0][0])\n",
    "# unigrams = pd.DataFrame({dterms[0][0]: [0]})\n",
    "\n",
    "# for i in range(news.shape[0]):\n",
    "#     if i > 0:\n",
    "#         tdf = pd.DataFrame([[0]*len(unigrams.columns)], columns=unigrams.columns)\n",
    "#         t = unigrams.append(tdf,ignore_index=True)\n",
    "#         unigrams = t\n",
    "#     for word in dterms[i]:\n",
    "#         if word not in words_encountered:\n",
    "#             words_encountered.add(word)\n",
    "#             col_index = len(unigrams.columns)\n",
    "#             temp = pd.DataFrame({word: [0]*(i+1)})\n",
    "#             t2 = unigrams.join(temp)\n",
    "#             unigrams = t2\n",
    "#             unigrams.at[i, word] = 1\n",
    "#         else:\n",
    "#             cols = list(unigrams.columns)\n",
    "#             unigrams.at[i, word] += 1\n",
    "# too slow with dataframes ^ \n",
    "\n",
    "# for i in range(10):\n",
    "#     print(dterms[i][:6])\n",
    "\n",
    "# unigrams.to_csv('unigrams2.csv', encoding='utf-8')\n",
    "\n",
    "# script for creating term index csv\n",
    "# for k in dterms.keys():\n",
    "#     print(k, dterms[k][0])\n",
    "\n",
    "# a_file = open(\"TermDocMatrix.csv\", \"w\")\n",
    "\n",
    "# writer = csv.writer(a_file)\n",
    "# for key, value in term_index.items():\n",
    "#     writer.writerow([key, value])\n",
    "\n",
    "# a_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
