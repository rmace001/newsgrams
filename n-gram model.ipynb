{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "# from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "porter = PorterStemmer()\n",
    "# #########################\n",
    "# DATA CLEANING TECHNIQUES\n",
    "# #########################\n",
    "# split by sentance\n",
    "# remove puncuation\n",
    "# lowercase letters\n",
    "# remove/convert numbers\n",
    "# -------------------------\n",
    "\n",
    "# #########################\n",
    "# NLP TECHNIQUES\n",
    "# #########################\n",
    "# stemming\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stopwords = {\"I\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"that\", \"thats\", \"that's\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\", \"now\"}\n",
    "# sent1 = \"i am sam\"\n",
    "# sent2 = \"sam i am\"\n",
    "# sent3 = \"i do not like green eggs and ham\"\n",
    "# sent4 = \"the quick person did not realize his speed and the quick person bumped \"\n",
    "\n",
    "\n",
    "# # words = re.findall(\"\\w+\", sent4)\n",
    "# # bigramCount = dict(Counter(zip(words, islice(words, 1, None))))\n",
    "\n",
    "# # for i in bigramCount.keys():\n",
    "# #     print(str(i) + \" count: %d\" % bigramCount[i])\n",
    "\n",
    "# load in the crawled data and perform some cleaning\n",
    "news = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\newsutf8.csv\")\n",
    "print(news.shape[0])\n",
    "news = news[:100]\n",
    "for i in range(news.shape[0]):\n",
    "    s = news.iloc[i][3][2:]\n",
    "    s2 = news.iloc[i][1][2:]\n",
    "    news.at[i,'body'] = s\n",
    "    news.at[i,'headline'] = s2\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "# print(news.iloc[0][1])\n",
    "# print(news.iloc[1][1])\n",
    "# print(news.iloc[2][1])\n",
    "# print(news.iloc[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n"
     ]
    }
   ],
   "source": [
    "gibberish = ['\\(CNN\\)', 'CNN', 'Getty Images', '\\\\\\\\xc2', '\\\\\\\\xb0', '\\\\\\\\xc3', '\\\\\\\\x94', '\\\\\\\\x9c', '\\\\\\\\x9d', '\\\\\\\\x95', '\\\\\\\\xb3', '\\\\\\\\xe2', '\\\\\\\\x80', '\\\\\\\\x99', '\\\\\\\\xa9', '\\\\\\\\xf0', '\\\\\\\\x9f', '\\\\\\\\x91']\n",
    "for i in range(len(gibberish)):\n",
    "    for j in range(news.shape[0]):\n",
    "        output = news.iloc[j][3]\n",
    "        output = re.sub(gibberish[i], '', output)\n",
    "        output = re.sub('\\\\\\\\n', ' ', output)\n",
    "        output = re.sub('\\\\\\\\\\\\\\'', '', output)\n",
    "        output = re.sub('\\'', '', output)\n",
    "        output = re.sub('Read More', '', output)\n",
    "\n",
    "        news.at[j,'body'] = output\n",
    "\n",
    "print(\"#################################################\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = str.maketrans('', '', string.punctuation)\n",
    "dterms = dict()\n",
    "for j in range(news.shape[0]):    \n",
    "    tokens = word_tokenize(news.iloc[j][3])\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    depunc = [t.translate(collection) for t in tokens]\n",
    "    words = [w for w in depunc if not w  == '']\n",
    "    cleaner = [w for w in words if not w in stopwords]\n",
    "    stemmed = [porter.stem(word) for word in cleaner]\n",
    "    dterms[str(j)+news.iloc[j][1]] = stemmed\n",
    "#     dterms[j] = cleaner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = []\n",
    "doc_index = {}\n",
    "term_index = {}\n",
    "words_per_doc = {}\n",
    "\n",
    "\n",
    "#function that creates termlist and doc_idx as we count each doc_length\n",
    "def populate_data_structs():\n",
    "#     for doc in range(len(dterms)):\n",
    "    for doc in dterms.keys():\n",
    "        count = 0\n",
    "        words_per_doc[doc] = set({})\n",
    "        for word in dterms[doc]:\n",
    "            count += 1\n",
    "            term_list.append((word, doc))\n",
    "            words_per_doc[doc].add(word)\n",
    "        doc_index[doc] = count\n",
    "\n",
    "def update_term(item):\n",
    "    i = 0\n",
    "    for listitem in term_index[item[0]]:\n",
    "        #if word_docName == encounterd_fileID\n",
    "        if (item[1] == listitem[0]):\n",
    "            listitem[1] += 1\n",
    "            return\n",
    "        else:\n",
    "            i += 1\n",
    "    term_index[item[0]].append( [item[1] , 1 ] )\n",
    "    return\n",
    "\n",
    "\n",
    "def make_term_index():\n",
    "    for item in term_list:\n",
    "        if item[0] not in term_index:\n",
    "            # add term\n",
    "            term_index[item[0]] = [[item[1], 1]]\n",
    "        else:\n",
    "            # update term\n",
    "            update_term(item)\n",
    "            \n",
    "populate_data_structs()\n",
    "make_term_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"0This Alaskan man is making a 14-hour boat trip to Costco every week to supply his small city with groceries amid the pandemic'\", \"1State Department inspector general becomes the latest watchdog fired by Trump'\", '2Inside New York\\'s notorious Rikers Island jails, \\'the epicenter of the epicenter\\' of the coronavirus pandemic\"', '3Warren Buffett\\'s Berkshire Hathaway sells majority of stake in Goldman Sachs\"', '4Bundesliga marks soccer\\'s elite return, with changes, during coronavirus pandemic\"', \"5Mass gatherings are making their way back into US life but may not look the way we remember'\", \"6CNN to air two-hour special honoring the class of 2020'\", '7Protectors of the French language says \\'Covid\\' takes a feminine article\"', \"8Tensions rise between the White House and CDC as Birx critiques virus tracking'\", '9Why a positive Covid-19 antibody test doesn\\'t mean much of anything yet\"', \"10Millennials are facing another once-in-a-generation economic disaster'\", \"11New York tourist is arrested in Hawaii after posting beach pictures on Instagram'\", '12Can they whip it? Devo offers \\'energy dome\\' face shields to fight the pandemic\"', \"13New York barber working illicitly tests positive for Covid-19'\", \"14Trump is back at Camp David as America -- and the West Wing -- tries to contain the virus'\", '15White House ramps up PR campaign to improve Trump\\'s image\"', \"16These are the states with coronavirus cases falling and rising the most'\", \"17Who Trump has removed from the inspector general role'\", \"18Watch CNN Go There'\", \"19This viral video of Trump supporters screaming at a reporter is a Rorschach test of America right now'\", '20Gulp, I think I\\'m ready to send my kids back to school\"', \"21Cornered by the coronavirus, Trump returns to a familiar strategy: Attack Obama'\", \"2224 Indian migrant workers killed in traffic accident while trying to get home during lockdown'\", '23I\\'m a black man in a Covid-19 hotspot. I don\\'t have sympathy for people of color who won\\'t social distance\"', \"24Spain reports lowest daily deaths from Covid-19 since mid-March'\", \"25NYPD officers arrest woman for allegedly striking officer after she did not wear face mask as required, authorities say'\", \"26Joe Biden needs black voters to win the presidency. And Covid-19 is making them even harder to reach'\", \"27Dozens of surrogacy babies stranded by coronavirus lockdown in Ukraine, lawmaker says'\", '28Why \\'Mulan\\' and other summer blockbusters won\\'t be coming to a home near you\"', \"29What you need to know about coronavirus on Saturday, May 16'\", \"30Her college canceled graduation. So her dad held an elaborate ceremony in the front yard'\", \"31After heart and kidney transplant, Florida man survives coronavirus'\", '32Mass testing won\\'t happen in Mexico. That\\'s the way the government wants it\"', '33Gary Lineker on football returning: \\'You cannot play football and social distance\\'\"', \"34Two Sikh doctors shaved their beards, a pillar of their religion, to continue treating Covid-19 patients'\", \"35Preschools and childcare might reopen soon. Should you send your kid?'\", \"36Texas Supreme Court halts counties from issuing mail-in ballots to voters afraid of virus'\", \"37Samuel L. Jackson, Ciara, Common and others send messages of hope to underserved communities hit by coronavirus'\", '38Taiwan\\'s success in fighting coronavirus has bolstered its global standing. This has infuriated Beijing\"', \"39JCPenney files for bankruptcy'\", \"40US coronavirus deaths to surpass 100,000 by June 1, CDC director says'\", \"41Even as grocery stores limit meat sales, US farmers may have to euthanize 10 million pigs'\", \"42House approves $3 trillion Covid aid bill and historic rules change to allow remote voting'\", \"43TSA working on plan to check temperatures at some American airports'\", '44It\\'s time to end zombie arguments against paid sick leave\"', '45Social media rules. That\\'s bad in a pandemic\"', '46Samuel Eto\\'o celebrated for more than football\"', '47How to watch CNN\\'s special on coronavirus and race\"', \"48Unveiling vaccine effort, Trump says country will be back with or without one'\", \"49Mary-Kate Olsen files for divorce from Olivier Sarkozy, emergency petition denied'\", \"50Top Trump campaign aide pushed anti-gay positions, said legalizing same-sex marriage would lead to bestiality and pedophilia'\", '51Acting intelligence chief has declassified names of Obama officials who \\'unmasked\\' Flynn\"', '52GOP congressman on why he\\'s not wearing a mask: \\'There\\'s just no need\\'\"', \"53Veterans who sacrificed for their country battle coronavirus threat'\", \"54Hawaii discourages tourists from coming to the state through at least the end of June'\", \"55NYC subway workers had plenty to do before a TikTok prankster dumped milk and cereal in a train'\", \"56White House announces new additions to coronavirus task force'\", \"57High school students who took AP exams online may have to retake them because of a glitch'\", \"58LeBron James and Travis Scott team up to design a T-shirt for the class of 2020'\", \"59Trump is ready to move on from coronavirus'\", '60\\'He lied on national television\\': Trump falsely claims truckers protesting industry problems are honking to support him\"', \"61Three weeks after opening, Georgia business owners chart their own course forward'\", \"62Pandemic jerk: A new kind of bad behavior'\", \"63Rick Bright plans to start new coronavirus treatments position, but his exact role is in dispute'\", '64The Future of Surveillance: Dr. Sanjay Gupta\\'s podcast for May 15\"', '65\\'Scoob!\\' isn\\'t much fun, and neither are those meddling kids\"', \"66The US has allowed just two migrants to stay under tougher border measures'\", \"67Officials raise concerns about CDC counting systems'\", '68Noma, one of the world\\'s best restaurants, to return as a wine bar\"', \"69Keith Urban treats healthcare workers to a private, drive-in concert'\", \"70Science speeds up during coronavirus pandemic -- but at what cost?'\", '71It\\'s not just Amazon: Why the pandemic will make Aldi and Dollar General more dominant\"', '72Contact tracing 101: How it works, who could get hired, and why it\\'s so critical in fighting coronavirus now\"', '73Treasury says most small businesses won\\'t have to fear audits on relief loans\"', \"74Preparing the palace: How an iconic Las Vegas casino plans to conquer Covid-19'\", \"75Nearly 40% of low-income workers lost their jobs in March'\", \"76German soccer returns but not as we know it'\", \"77Biden campaign ramps up battleground operations and eyes a wide 2020 map'\", '78See the gardens of famous designers as Britain\\'s prestigious Chelsea Flower Show goes virtual\"', '79Amazon insists sharing data on coronavirus cases in its warehouses isn\\'t useful\"', \"80JCPenney buys itself a little more time'\", \"81The pandemic could drive homelessness up as much as 45%, an economist projects'\", \"82China is mobilizing its global media machine in the coronavirus war of words'\", '83Traveling amid coronavirus is like nothing I\\'ve seen before\"', \"84German cafe tells customers to wear pool noodles to enforce social distancing'\", \"85US pushes for new crackdown on Huawei, raising concerns of retaliation against American companies'\", '86Michigan governor says legislators \\'didn\\'t want to be around\\' for anti-lockdown protests \\'they incited\\'\"', \"87Remote destinations for luxurious isolation'\", \"88Trump campaign laying groundwork for a return to in-person rallies'\", \"89Amid WHO warnings and with no proof, some African nations turn to herbal tonic to try to treat Covid-19'\", \"90The once-disappearing phone call is making a comeback'\", \"91Dear Grandma, read this before you visit the little ones'\", \"92Over 100 experts are calling on states to mandate masks, face coverings'\", '93America\\'s retail sales completely collapsed in April\"', \"94Advice to seniors from seniors about graduating in the time of a national crisis'\", '95Brazil\\'s Amazon faces deforestation and Covid-19 at once\"', \"96Live sports are mostly gone, but betting is still big business'\", \"97The CDC is a national treasure. Why is it being sidelined?'\", \"98Top health officials vanish from national TV interviews as White House refocuses messaging'\", '99For 200 years, villagers lived 2,600 feet up a cliff. Now they\\'re in a housing estate\"']\n"
     ]
    }
   ],
   "source": [
    "def termweight(term, docno):\n",
    "    if term in words_per_doc[docno]:\n",
    "        for postings in term_index[term]:\n",
    "            if postings[0] == docno:\n",
    "#                 tf = postings[1]/doc_index[postings[0]] #num_occur/doc_size\n",
    "#                 idf = math.log(len(doc_index)/len(term_index[term]), 2)\n",
    "#                 tf_idf = tf*idf\n",
    "#                 print('Posting: Doc# ' + str(postings[0]) + ', tf: ' + str(tf) + ', idf: ' + str(idf) + ', tf-idf: ' + str(tf_idf))\n",
    "                return postings[1]/doc_index[postings[0]] * math.log(len(doc_index)/len(term_index[term]), 2)\n",
    "#     print('Posting: Doc# ' + str(docno) + ', tf: ' + str(0))\n",
    "    return 0\n",
    "\n",
    "\n",
    "term_doc_matrix = {}\n",
    "\n",
    "for term in term_index.keys():\n",
    "    row = []\n",
    "    for doc in doc_index.keys():\n",
    "        row.append(termweight(term, doc))\n",
    "    term_doc_matrix[term] = row\n",
    "\n",
    "\n",
    "keys = list(doc_index.keys())\n",
    "print(keys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(term_doc_matrix, orient='index', columns=keys)\n",
    "\n",
    "df.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\tfidfFirst100.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posting_list(term):\n",
    "    return term_index[term] if term in term_index else []\n",
    "\n",
    "def get_q_weight(term, query_words):\n",
    "    tf = 0\n",
    "    for w in query_words:\n",
    "        if w.lower() == term:\n",
    "            tf += 1\n",
    "    tf = tf / len(query_words)\n",
    "    t = ps.stem(term)\n",
    "    if t in term_index:\n",
    "        idf = 1 + math.log(len(doc_index)/len(term_index[t]))\n",
    "    else:\n",
    "        idf = 1\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "\n",
    "def get_d_weight(term, pair):\n",
    "    if term in term_index:\n",
    "        #postings = self.term_index[term]\n",
    "        tf = pair[1]/doc_index[pair[0]] #num_occur/doc_size\n",
    "        idf = 1 + math.log(len(doc_index)/len(term_index[term]))\n",
    "        tf_idf = tf * idf\n",
    "    else:\n",
    "        tf_idf = 0\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def cosine_score(query_words, inv_idx):\n",
    "    scores = dict()\n",
    "    for doc in inv_idx.doc_index.keys():\n",
    "        scores[doc] = 0\n",
    "    \n",
    "    for t in query_words:\n",
    "        t = t.lower()\n",
    "        term = ps.stem(t)\n",
    "        postings = inv_idx.get_posting_list(term)\n",
    "        q_t_weight = inv_idx.get_q_weight(t, query_words)\n",
    "\n",
    "\n",
    "        for pair in postings:\n",
    "            d_t_weight = inv_idx.get_d_weight(term, pair)\n",
    "            scores[pair[0]] += q_t_weight * d_t_weight\n",
    "        \n",
    "    for doc in inv_idx.doc_index.keys():\n",
    "        scores[doc] = scores[doc] / inv_idx.doc_index[doc]\n",
    "        \n",
    "    sorted_scores = sorted(scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the set and the dataframe with the first word of the first document\n",
    "# words_encountered = set(dterms[0][0])\n",
    "# unigrams = pd.DataFrame({dterms[0][0]: [0]})\n",
    "\n",
    "# for i in range(news.shape[0]):\n",
    "#     if i > 0:\n",
    "#         tdf = pd.DataFrame([[0]*len(unigrams.columns)], columns=unigrams.columns)\n",
    "#         t = unigrams.append(tdf,ignore_index=True)\n",
    "#         unigrams = t\n",
    "#     for word in dterms[i]:\n",
    "#         if word not in words_encountered:\n",
    "#             words_encountered.add(word)\n",
    "#             col_index = len(unigrams.columns)\n",
    "#             temp = pd.DataFrame({word: [0]*(i+1)})\n",
    "#             t2 = unigrams.join(temp)\n",
    "#             unigrams = t2\n",
    "#             unigrams.at[i, word] = 1\n",
    "#         else:\n",
    "#             cols = list(unigrams.columns)\n",
    "#             unigrams.at[i, word] += 1\n",
    "# too slow with dataframes ^ \n",
    "\n",
    "# for i in range(10):\n",
    "#     print(dterms[i][:6])\n",
    "\n",
    "# unigrams.to_csv('unigrams2.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in dterms.keys():\n",
    "#     print(k, dterms[k][0])\n",
    "\n",
    "a_file = open(\"TermDocMatrix.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in term_index.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "a_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
