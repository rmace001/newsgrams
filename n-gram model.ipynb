{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "# from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "porter = PorterStemmer()\n",
    "# #########################\n",
    "# DATA CLEANING TECHNIQUES\n",
    "# #########################\n",
    "# split by sentance\n",
    "# remove puncuation\n",
    "# lowercase letters\n",
    "# remove/convert numbers\n",
    "# -------------------------\n",
    "\n",
    "# #########################\n",
    "# NLP TECHNIQUES\n",
    "# #########################\n",
    "# stemming\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stopwords = {\"I\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"that\", \"thats\", \"that's\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\", \"now\"}\n",
    "# sent1 = \"i am sam\"\n",
    "# sent2 = \"sam i am\"\n",
    "# sent3 = \"i do not like green eggs and ham\"\n",
    "# sent4 = \"the quick person did not realize his speed and the quick person bumped \"\n",
    "\n",
    "\n",
    "# # words = re.findall(\"\\w+\", sent4)\n",
    "# # bigramCount = dict(Counter(zip(words, islice(words, 1, None))))\n",
    "\n",
    "# # for i in bigramCount.keys():\n",
    "# #     print(str(i) + \" count: %d\" % bigramCount[i])\n",
    "\n",
    "# load in the crawled data and perform some cleaning\n",
    "news = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\newsutf8.csv\")\n",
    "print(news.shape[0])\n",
    "news = news[:100]\n",
    "for i in range(news.shape[0]):\n",
    "    s = news.iloc[i][3][2:]\n",
    "    news.at[i,'body'] = s\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n"
     ]
    }
   ],
   "source": [
    "gibberish = ['\\(CNN\\)', 'CNN', 'Getty Images', '\\\\\\\\xc2', '\\\\\\\\xb0', '\\\\\\\\xc3', '\\\\\\\\x94', '\\\\\\\\x9c', '\\\\\\\\x9d', '\\\\\\\\x95', '\\\\\\\\xb3', '\\\\\\\\xe2', '\\\\\\\\x80', '\\\\\\\\x99', '\\\\\\\\xa9', '\\\\\\\\xf0', '\\\\\\\\x9f', '\\\\\\\\x91']\n",
    "for i in range(len(gibberish)):\n",
    "    for j in range(news.shape[0]):\n",
    "        output = news.iloc[j][3]\n",
    "        output = re.sub(gibberish[i], '', output)\n",
    "        output = re.sub('\\\\\\\\n', ' ', output)\n",
    "        output = re.sub('\\\\\\\\\\\\\\'', '', output)\n",
    "        output = re.sub('\\'', '', output)\n",
    "        output = re.sub('Read More', '', output)\n",
    "\n",
    "        news.at[j,'body'] = output\n",
    "\n",
    "print(\"#################################################\")\n",
    "# for i in range(news.shape[0]):\n",
    "#     print(news.at[i, 'body'])\n",
    "#     print(\"#################################################\")\n",
    "# print(news.iloc[-1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = str.maketrans('', '', string.punctuation)\n",
    "dterms = dict()\n",
    "for j in range(news.shape[0]):    \n",
    "    tokens = word_tokenize(news.iloc[j][3])\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    depunc = [t.translate(collection) for t in tokens]\n",
    "    words = [w for w in depunc if not w  == '']\n",
    "    cleaner = [w for w in words if not w in stopwords]\n",
    "#     stemmed = [porter.stem(word) for word in cleaner]\n",
    "#     dterms[j] = stemmed\n",
    "    dterms[j] = cleaner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = []\n",
    "doc_index = {}\n",
    "term_index = {}\n",
    "words_per_doc = {}\n",
    "\n",
    "\n",
    "#function that creates termlist and doc_idx as we count each doc_length\n",
    "def populate_data_structs():\n",
    "    for doc in range(len(dterms)):\n",
    "        count = 0\n",
    "        words_per_doc[doc] = set({})\n",
    "        for word in dterms[doc]:\n",
    "            count += 1\n",
    "            term_list.append((word, doc))\n",
    "            words_per_doc[doc].add(word)\n",
    "        doc_index[doc] = count\n",
    "\n",
    "def update_term(item):\n",
    "    i = 0\n",
    "    for listitem in term_index[item[0]]:\n",
    "        #if word_docName == encounterd_fileID\n",
    "        if (item[1] == listitem[0]):\n",
    "            listitem[1] += 1\n",
    "            return\n",
    "        else:\n",
    "            i += 1\n",
    "    term_index[item[0]].append( [item[1] , 1 ] )\n",
    "    return\n",
    "\n",
    "\n",
    "def make_term_index():\n",
    "    for item in term_list:\n",
    "        if item[0] not in term_index:\n",
    "            # add term\n",
    "            term_index[item[0]] = [[item[1], 1]]\n",
    "        else:\n",
    "            # update term\n",
    "            update_term(item)\n",
    "            \n",
    "populate_data_structs()\n",
    "make_term_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "def termweight(term, docno):\n",
    "    if term in words_per_doc[docno]:\n",
    "        for postings in term_index[term]:\n",
    "            if postings[0] == docno:\n",
    "#                 tf = postings[1]/doc_index[postings[0]] #num_occur/doc_size\n",
    "#                 idf = math.log(len(doc_index)/len(term_index[term]), 2)\n",
    "#                 tf_idf = tf*idf\n",
    "#                 print('Posting: Doc# ' + str(postings[0]) + ', tf: ' + str(tf) + ', idf: ' + str(idf) + ', tf-idf: ' + str(tf_idf))\n",
    "                return postings[1]/doc_index[postings[0]] * math.log(len(doc_index)/len(term_index[term]), 2)\n",
    "#     print('Posting: Doc# ' + str(docno) + ', tf: ' + str(0))\n",
    "    return 0\n",
    "\n",
    "\n",
    "term_doc_matrix = {}\n",
    "\n",
    "for term in term_index.keys():\n",
    "    row = []\n",
    "    for doc in doc_index.keys():\n",
    "        row.append(termweight(term, doc))\n",
    "    term_doc_matrix[term] = row\n",
    "\n",
    "\n",
    "keys = list(doc_index.keys())\n",
    "print(keys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(term_doc_matrix, orient='index', columns=keys)\n",
    "\n",
    "df.to_csv (r'C:\\Users\\Rogith\\Desktop\\Notebooks\\newsgrams\\tfidfFirst100.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posting_list(term):\n",
    "    return term_index[term] if term in term_index else []\n",
    "\n",
    "def get_q_weight(term, query_words):\n",
    "    tf = 0\n",
    "    for w in query_words:\n",
    "        if w.lower() == term:\n",
    "            tf += 1\n",
    "    tf = tf / len(query_words)\n",
    "    t = ps.stem(term)\n",
    "    if t in term_index:\n",
    "        idf = 1 + math.log(len(doc_index)/len(term_index[t]))\n",
    "    else:\n",
    "        idf = 1\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "\n",
    "def get_d_weight(term, pair):\n",
    "    if term in term_index:\n",
    "        #postings = self.term_index[term]\n",
    "        tf = pair[1]/doc_index[pair[0]] #num_occur/doc_size\n",
    "        idf = 1 + math.log(len(doc_index)/len(term_index[term]))\n",
    "        tf_idf = tf * idf\n",
    "    else:\n",
    "        tf_idf = 0\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def cosine_score(query_words, inv_idx):\n",
    "    scores = dict()\n",
    "    for doc in inv_idx.doc_index.keys():\n",
    "        scores[doc] = 0\n",
    "    \n",
    "    for t in query_words:\n",
    "        t = t.lower()\n",
    "        term = ps.stem(t)\n",
    "        postings = inv_idx.get_posting_list(term)\n",
    "        q_t_weight = inv_idx.get_q_weight(t, query_words)\n",
    "\n",
    "\n",
    "        for pair in postings:\n",
    "            d_t_weight = inv_idx.get_d_weight(term, pair)\n",
    "            scores[pair[0]] += q_t_weight * d_t_weight\n",
    "        \n",
    "    for doc in inv_idx.doc_index.keys():\n",
    "        scores[doc] = scores[doc] / inv_idx.doc_index[doc]\n",
    "        \n",
    "    sorted_scores = sorted(scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the set and the dataframe with the first word of the first document\n",
    "# words_encountered = set(dterms[0][0])\n",
    "# unigrams = pd.DataFrame({dterms[0][0]: [0]})\n",
    "\n",
    "# for i in range(news.shape[0]):\n",
    "#     if i > 0:\n",
    "#         tdf = pd.DataFrame([[0]*len(unigrams.columns)], columns=unigrams.columns)\n",
    "#         t = unigrams.append(tdf,ignore_index=True)\n",
    "#         unigrams = t\n",
    "#     for word in dterms[i]:\n",
    "#         if word not in words_encountered:\n",
    "#             words_encountered.add(word)\n",
    "#             col_index = len(unigrams.columns)\n",
    "#             temp = pd.DataFrame({word: [0]*(i+1)})\n",
    "#             t2 = unigrams.join(temp)\n",
    "#             unigrams = t2\n",
    "#             unigrams.at[i, word] = 1\n",
    "#         else:\n",
    "#             cols = list(unigrams.columns)\n",
    "#             unigrams.at[i, word] += 1\n",
    "# too slow with dataframes ^ \n",
    "\n",
    "# for i in range(10):\n",
    "#     print(dterms[i][:6])\n",
    "\n",
    "# unigrams.to_csv('unigrams2.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in dterms.keys():\n",
    "#     print(k, dterms[k][0])\n",
    "\n",
    "a_file = open(\"TermDocMatrix.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in term_index.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "a_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
