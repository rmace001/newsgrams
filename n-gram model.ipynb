{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "# #########################\n",
    "# DATA CLEANING TECHNIQUES\n",
    "# #########################\n",
    "# split by sentance\n",
    "# remove puncuation\n",
    "# lowercase letters\n",
    "# remove/convert numbers\n",
    "# -------------------------\n",
    "\n",
    "# #########################\n",
    "# NLP TECHNIQUES\n",
    "# #########################\n",
    "# stemming\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {\"I\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"}\n",
    "# sent1 = \"i am sam\"\n",
    "# sent2 = \"sam i am\"\n",
    "# sent3 = \"i do not like green eggs and ham\"\n",
    "# sent4 = \"the quick person did not realize his speed and the quick person bumped \"\n",
    "\n",
    "\n",
    "# # words = re.findall(\"\\w+\", sent4)\n",
    "# # bigramCount = dict(Counter(zip(words, islice(words, 1, None))))\n",
    "\n",
    "# # for i in bigramCount.keys():\n",
    "# #     print(str(i) + \" count: %d\" % bigramCount[i])\n",
    "\n",
    "# load in the crawled data and perform some cleaning\n",
    "news = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\news2.csv\")\n",
    "news = news[:100]\n",
    "for i in range(news.shape[0]):\n",
    "    s = news.iloc[i][4][2:]\n",
    "    news.iloc[i][4] = s\n",
    "\n",
    "# s = news.iloc[23][4]\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n"
     ]
    }
   ],
   "source": [
    "gibberish = ['Getty Images', '\\\\\\\\xc3', '\\\\\\\\x94', '\\\\\\\\x9c', '\\\\\\\\x9d', '\\\\\\\\x95', '\\\\\\\\xb3', '\\\\\\\\xe2', '\\\\\\\\x80', '\\\\\\\\x99', '\\\\\\\\xa9', '\\\\\\\\xf0', '\\\\\\\\x9f', '\\\\\\\\x91']\n",
    "for i in range(len(gibberish)):\n",
    "    for j in range(news.shape[0]):\n",
    "        output = news.iloc[j][4]\n",
    "        output = re.sub(gibberish[i], '', output)\n",
    "        output = re.sub('\\\\\\\\n', ' ', output)\n",
    "        output = re.sub('\\\\\\\\\\\\\\'', '\\'', output)\n",
    "        news.iloc[j][4] = output\n",
    "\n",
    "print(\"#################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = str.maketrans('', '', string.punctuation)\n",
    "dterms = dict()\n",
    "for j in range(news.shape[0]):    \n",
    "    tokens = word_tokenize(news.iloc[j][4])\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    depunc = [t.translate(collection) for t in tokens]\n",
    "    words = [w for w in depunc if not w  == '']\n",
    "    cleaner = [w for w in words if not w in stopwords]\n",
    "    stemmed = [porter.stem(word) for word in cleaner]\n",
    "    dterms[j] = stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print when finished\n"
     ]
    }
   ],
   "source": [
    "# initialize the set and the dataframe with the first word of the first document\n",
    "words_encountered = set(dterms[0][0])\n",
    "unigrams = pd.DataFrame({dterms[0][0]: [0]})\n",
    "\n",
    "for i in range(news.shape[0]):\n",
    "# for i in range(4):\n",
    "    #create new row in unigrams\n",
    "    if i > 0:\n",
    "        tdf = pd.DataFrame([[0]*len(unigrams.columns)], columns=unigrams.columns)\n",
    "        t = unigrams.append(tdf,ignore_index=True)\n",
    "        unigrams = t\n",
    "#     j = 0\n",
    "    for word in dterms[i]:\n",
    "        if word not in words_encountered:\n",
    "            words_encountered.add(word)\n",
    "            col_index = len(unigrams.columns)\n",
    "            temp = pd.DataFrame({word: [0]*(i+1)})\n",
    "            t2 = unigrams.join(temp)\n",
    "            unigrams = t2\n",
    "            unigrams.at[i, word] = 1\n",
    "        else:\n",
    "            cols = list(unigrams.columns)\n",
    "            unigrams.at[i, word] += 1\n",
    "# print(unigrams)\n",
    "#         j += 1\n",
    "#         if j == 3:\n",
    "#             break\n",
    "print(\"print when finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams.to_csv('unigrams.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cnn\n",
      "1 cnn\n",
      "2 cnn\n",
      "3 cnn\n",
      "4 cnn\n",
      "5 cnn\n",
      "6 john\n",
      "7 hsu\n",
      "8 cnn\n",
      "9 miller\n",
      "10 cnn\n",
      "11 cnn\n",
      "12 washington\n",
      "13 cnn\n",
      "14 cnn\n",
      "15 cnn\n",
      "16 chat\n",
      "17 dubai\n",
      "18 coronaviru\n",
      "19 cnn\n",
      "20 cnn\n",
      "21 cnn\n",
      "22 former\n",
      "23 cnn\n",
      "24 new\n",
      "25 nadia\n",
      "26 cnn\n",
      "27 version\n",
      "28 cnn\n",
      "29 cnn\n",
      "30 cnn\n",
      "31 roy\n",
      "32 glynn\n",
      "33 say\n",
      "34 cnn\n",
      "35 cnn\n",
      "36 covid\n",
      "37 cnn\n",
      "38 cnn\n",
      "39 cnn\n",
      "40 washington\n",
      "41 cnn\n",
      "42 julian\n",
      "43 cnn\n",
      "44 cnn\n",
      "45 cnn\n",
      "46 cnn\n",
      "47 cnn\n",
      "48 peopl\n",
      "49 former\n",
      "50 cnn\n",
      "51 new\n",
      "52 cnn\n",
      "53 glynn\n",
      "54 desert\n",
      "55 offic\n",
      "56 cnn\n",
      "57 washington\n",
      "58 chat\n",
      "59 cnn\n",
      "60 cnn\n",
      "61 cnn\n",
      "62 state\n",
      "63 month\n",
      "64 washington\n",
      "65 cnn\n",
      "66 analysi\n",
      "67 cnn\n",
      "68 cnn\n",
      "69 cnn\n",
      "70 cnn\n",
      "71 cnn\n",
      "72 chat\n",
      "73 san\n",
      "74 man\n",
      "75 cnn\n",
      "76 cnn\n",
      "77 cnn\n",
      "78 recent\n",
      "79 cnn\n",
      "80 david\n",
      "81 cnn\n",
      "82 cnn\n",
      "83 cnn\n",
      "84 washington\n",
      "85 cnn\n",
      "86 mindauga\n",
      "87 chat\n",
      "88 cnn\n",
      "89 new\n",
      "90 cnn\n",
      "91 seattl\n",
      "92 raleigh\n",
      "93 cnn\n",
      "94 much\n",
      "95 cnn\n",
      "96 new\n",
      "97 new\n",
      "98 san\n",
      "99 cnn\n"
     ]
    }
   ],
   "source": [
    "for k in dterms.keys():\n",
    "    print(k, dterms[k][0])\n",
    "\n",
    "a_file = open(\"clean.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in dterms.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "a_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
