{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "# stemmed = [porter.stem(word) for word in tokens]\n",
    "# #########################\n",
    "# DATA CLEANING TECHNIQUES\n",
    "# #########################\n",
    "# split by sentance\n",
    "# remove puncuation\n",
    "# lowercase letters\n",
    "# remove/convert numbers\n",
    "# -------------------------\n",
    "\n",
    "# #########################\n",
    "# NLP TECHNIQUES\n",
    "# #########################\n",
    "# stemming\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>URL</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Study shows Covid-19 patients who took heart...</td>\n",
       "      <td>https://www.cnn.com/2020/05/09/health/famotidi...</td>\n",
       "      <td>2020-05-09 00:00:00</td>\n",
       "      <td>['Elizabeth Cohen', 'Dr. Minali Nigam']</td>\n",
       "      <td>b'(CNN) Patients who took a common heartburn m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Trump increasingly engaged in legal battles ...</td>\n",
       "      <td>https://www.cnn.com/2020/05/09/politics/trump-...</td>\n",
       "      <td>2020-05-09 00:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>b\"(CNN) President Donald Trump has been increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'3 New York youths died of an inflammatory sy...</td>\n",
       "      <td>https://www.cnn.com/2020/05/09/health/us-coron...</td>\n",
       "      <td>2020-05-09 00:00:00</td>\n",
       "      <td>['Madeline Holcombe', 'Jason Hanna']</td>\n",
       "      <td>b'(CNN) An inflammatory illness recently obser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'These are the people involved with the Ahmau...</td>\n",
       "      <td>https://www.cnn.com/2020/05/09/us/who-is-ahmau...</td>\n",
       "      <td>2020-05-09 00:00:00</td>\n",
       "      <td>['Dakin Andone']</td>\n",
       "      <td>b'(CNN) It\\'s been more than two months since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'How Covid-19 misinformation is still going v...</td>\n",
       "      <td>https://www.cnn.com/2020/05/08/tech/covid-vira...</td>\n",
       "      <td>2020-05-08 00:00:00</td>\n",
       "      <td>[\"Donie O'Sullivan\", 'Cnn Business']</td>\n",
       "      <td>b'(CNN Business) Despite pledges from the big ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  b'Study shows Covid-19 patients who took heart...   \n",
       "1  b'Trump increasingly engaged in legal battles ...   \n",
       "2  b'3 New York youths died of an inflammatory sy...   \n",
       "3  b'These are the people involved with the Ahmau...   \n",
       "4  b'How Covid-19 misinformation is still going v...   \n",
       "\n",
       "                                                 URL         Publish Date  \\\n",
       "0  https://www.cnn.com/2020/05/09/health/famotidi...  2020-05-09 00:00:00   \n",
       "1  https://www.cnn.com/2020/05/09/politics/trump-...  2020-05-09 00:00:00   \n",
       "2  https://www.cnn.com/2020/05/09/health/us-coron...  2020-05-09 00:00:00   \n",
       "3  https://www.cnn.com/2020/05/09/us/who-is-ahmau...  2020-05-09 00:00:00   \n",
       "4  https://www.cnn.com/2020/05/08/tech/covid-vira...  2020-05-08 00:00:00   \n",
       "\n",
       "                                   Authors  \\\n",
       "0  ['Elizabeth Cohen', 'Dr. Minali Nigam']   \n",
       "1                                       []   \n",
       "2     ['Madeline Holcombe', 'Jason Hanna']   \n",
       "3                         ['Dakin Andone']   \n",
       "4     [\"Donie O'Sullivan\", 'Cnn Business']   \n",
       "\n",
       "                                                Body  \n",
       "0  b'(CNN) Patients who took a common heartburn m...  \n",
       "1  b\"(CNN) President Donald Trump has been increa...  \n",
       "2  b'(CNN) An inflammatory illness recently obser...  \n",
       "3  b'(CNN) It\\'s been more than two months since ...  \n",
       "4  b'(CNN Business) Despite pledges from the big ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(type())\n",
    "stopwords = {\"I\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"}\n",
    "sent1 = \"i am sam\"\n",
    "sent2 = \"sam i am\"\n",
    "sent3 = \"i do not like green eggs and ham\"\n",
    "sent4 = \"the quick person did not realize his speed and the quick person bumped \"\n",
    "\n",
    "\n",
    "# words = re.findall(\"\\w+\", sent4)\n",
    "# bigramCount = dict(Counter(zip(words, islice(words, 1, None))))\n",
    "\n",
    "\n",
    "\n",
    "# for i in bigramCount.keys():\n",
    "#     print(str(i) + \" count: %d\" % bigramCount[i])\n",
    "# # \n",
    "\n",
    "# load in the crawled data and perform some cleaning\n",
    "news = pd.read_csv(r\"C:\\Users\\Rogith\\Desktop\\Notebooks\\news1.csv\")\n",
    "display(news.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n"
     ]
    }
   ],
   "source": [
    "gibberish = ['Getty Images', '\\\\\\\\xc3', '\\\\\\\\x94', '\\\\\\\\x9c', '\\\\\\\\x9d', '\\\\\\\\x95', '\\\\\\\\xb3', '\\\\\\\\xe2', '\\\\\\\\x80', '\\\\\\\\x99', '\\\\\\\\xa9', '\\\\\\\\xf0', '\\\\\\\\x9f', '\\\\\\\\x91']\n",
    "for i in range(len(gibberish)):\n",
    "    for j in range(news.shape[0]):\n",
    "        output = news.iloc[j][4]\n",
    "        output = re.sub(gibberish[i], '', output)\n",
    "        output = re.sub('\\\\\\\\n', ' ', output)\n",
    "        output = re.sub('\\\\\\\\\\\\\\'', '\\'', output)\n",
    "        news.iloc[j][4] = output\n",
    "\n",
    "print(\"#################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n",
      "['b', 'people', 'walk', 'madrid', 'friday', 'may', '8', 'javier', 'sorianoafp', '50', 'spains', 'population', 'advance', 'phase', 'one', 'monday', 'part', 'countrys', 'deescalation', 'process', 'coronavirus', 'crisis', 'spain', 's', 'health', 'minister', 'announced', 'friday', 'madrid', 'barcelona', 'two', 'largest', 'cities', 'country', 'hardest', 'hit', 'pandemic', 'staying', 'behind', 'nationallytelevised', 'news', 'conference', 'friday', 'evening', 'health', 'minister', 'salvador', 'illa', 'director', 'health', 'emergencies', 'dr', 'fernando', 'simn', 'said', 'government', 'decided', 'parts', 'country', 'could', 'advance', 'phase', 'one', 'consulting', 'spain', 's', '17', 'regional', 'governments', 'regarding', 'infection', 'rates', 'region', 'capacity', 'quickly', 'detect', 'new', 'cases', 'region', 's', 'hospitals', 'could', 'respond', 'second', 'wave', 'simn', 'listed', 'total', '11', 'regions', 'transition', 'fully', 'next', 'phase', 'galicia', 'asturias', 'cantabria', 'pais', 'vasco', 'la', 'rioja', 'navarra', 'aragon', 'extremadura', 'murcia', 'balearic', 'islands', 'canaries', 'spains', 'two', 'enclaves', 'moroccos', 'north', 'coast', 'ceuta', 'melilla', 'regions', 'castilla', 'leon', 'catalonia', 'castilla', 'la', 'mancha', 'valencia', 'andalusia', 'provinces', 'health', 'districts', 'advancing', 'phase', 'one', 'entire', 'region', 'madrid', 'region', 'includes', 'spanish', 'capital', 'surrounding', 'cities', 'requested', 'move', 'phase', 'one', 'health', 'minister', 'said', 'region', 'met', 'technical', 'criteria', 'yet', 'would', 'predict', 'could', 'advance', 'next', 'level', 'barcelona', 'spain', 's', 'second', 'largest', 'city', 'also', 'province', 'name', 'numerous', 'adjacent', 'cities', 'also', 'advance', 'phase', 'one', 'madrid', 'region', 'barcelona', 'province', 'special', 'challenges', 'density', 'population', 'movement', 'population', 'exchange', 'nationally', 'parts', 'spain', 'international', 'interaction', 'simn', 'added']\n"
     ]
    }
   ],
   "source": [
    "collection = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "tokens = word_tokenize(news.iloc[48][4])\n",
    "tokens = [t.lower() for t in tokens]\n",
    "depunc = [t.translate(collection) for t in tokens]\n",
    "words = [w for w in depunc if not w  == '']\n",
    "cleaner = [w for w in words if not w in stopwords]\n",
    "# print(\"#################################################\")\n",
    "# print(news.iloc[48][4])\n",
    "print(\"#################################################\")\n",
    "print(cleaner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
